{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation\n",
    "\n",
    "* Damilola Agbolabori\n",
    "* Obinna Onyema\n",
    "\n",
    "####  Emails:\n",
    "* dagbolabori@torontomu.ca\n",
    "* obinna.onyema@torontomu.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL tasks. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "LLMs require \"coaching\" or prompts to condition them to generate the correct responses. Prompt engineering for SQL tasks is tricky because there's syntax and noise challenges that LLMs struggle with.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Write a sentence or two about limitations of prior appraoches (you will provide their details in the background section next)\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "How the method you are discussing is going to solve it today. Wrtie couple of sentences only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dami Update\n",
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Text-to-SQL (Structured Query Language) is a challenging task in both natural language processing (NLP) and database communities. It involves translating natural language questions into SQL queries that can be executed on a given relational database. This task is essential for enabling non-expert users to interact with databases effectively, as it allows them to pose questions in natural language rather than requiring knowledge of SQL syntax.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "Traditionally, Text-to-SQL systems relied on predefined rules or query enumeration techniques to handle this task. However, with the advent of deep learning and large language models (LLMs), such as GPT (Generative Pre-trained Transformer) models, there has been a shift towards more data-driven approaches. These models can learn complex patterns and mappings between natural language questions and SQL queries, making them well-suited for Text-to-SQL tasks.\n",
    "\n",
    "The goal of Text-to-SQL research is to develop effective algorithms and models that can accurately understand and translate natural language questions into SQL queries. This involves various challenges, including handling ambiguity in natural language, understanding database schemas, and generating syntactically correct SQL queries.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Recent advancements in LLM-based Text-to-SQL have shown promising results, with models achieving high accuracy on benchmark datasets like Spider. However, there is still room for improvement, particularly in areas such as prompt engineering, example selection, and fine-tuning of LLMs for this specific task.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "This project suggests testing the GPT-4 model across the various prompt types. This approach aims to evaluate the effectiveness of different question representations with a consistent model, allowing for a direct comparison of their performance. The various prompt types that will be tested are listed below\n",
    "\n",
    "1. BSùëÉ (Baseline Prompt): This representation does not include any additional components like instruction, rule implication, or foreign keys.\n",
    "2. TRùëÉ (Task Representation Prompt): This representation includes task-specific instructions but excludes rule implication and foreign keys.\n",
    "3. ODùëÉ (Original Data Prompt): This representation includes task-specific instructions and foreign keys but excludes rule implication.\n",
    "4. CRùëÉ (Complete Rule Prompt): This representation includes task-specific instructions and rule implications but excludes foreign keys.\n",
    "5. ASùëÉ (All-in-One Prompt): This representation includes all components - task-specific instructions, rule implications, and foreign keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Yu et al. (2018) | They used a Seq2Seq model to create a tree based SQL decoder that can identify columns better as well as create nested queries| Spider dataset | Only 48% accuracy\n",
    "| Wang et al. (2020) | They focused on the medical domain considering the unique structure and terminology of medical records. They introduced the Translate-Edit Model for Question-to-SQL (TREQS) generation task by first generating the targeted SQL directly then editing with both attentive-copying mechanism and a recover technique| MIMICSQL | High accuracy of 85% but small dataset of 10k records\n",
    "| Dawei et al. (2023) | They focused on mplementing and refining two key methodologies from DAIL-SQL: DAIL Selection (DAILùëÜ) and DAIL Organization (DAILùëÇ). DAILùëÜ emphasizes candidate example selection based on question and query similarity, while DAILùëÇ focuses on efficient organization of examples while preserving question-SQL mapping. Our aim is to enhance Text-to-SQL systems by optimizing these methods for improved performance and accuracy.| Spider dataset | 86.2% \n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The study's methodology focuses on exclusively evaluating Text-to-SQL methods using the GPT-4 model, with the Spider datasets. Spider comprises extensive instances split into training and development sets, while Spider-Realistic offers a more challenging subset. Evaluation metrics encompass exact-set-match accuracy (EM) and execution accuracy (EX). The study solely leverages GPT-4, employing it alongside other LLMs for question representation, example selection, and organization strategies. Additionally, supervised fine-tuning with open-source LLMs is explored, considering diverse question representations, model scales, and alignment methods. Insights and guidelines are then derived from the experimental outcomes.\n",
    "\n",
    "In this project, our primary focus was on harnessing the capabilities of the GPT-4 model to enhance Text-to-SQL performance. We developed two key methodologies, DAIL Selection (DAILùëÜ) and DAIL Organization (DAILùëÇ), tailored specifically for GPT-4. DAILùëÜ selects examples based on questions and queries, while DAILùëÇ organizes examples to balance quality and quantity, preserving question-to-SQL mappings. Integrated into our approach, these methodologies aim to significantly elevate Text-to-SQL task execution accuracy.\n",
    "\n",
    "\n",
    "\n",
    "![Alternate text ](text-to-sql.png \"Title of the figure, location is simply the directory of the notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Write what you have learnt in this project. In particular, write few sentences about the results and their limitations, how they can be extended in future. Make sure your own inference/learnings are depicted here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Authors names, title of the paper, Conference Name,Year, page number (iff available)\n",
    "\n",
    "[2]:  Author names, title of the paper, Journal Name,Journal Vol, Issue Num, Year, page number (iff available)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
