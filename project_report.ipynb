{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation\n",
    "\n",
    "* Damilola Agbolabori (dagbolabori@torontomu.ca)\n",
    "* Obinna Onyema (obinna.onyema@torontomu.ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "Text-to-SQL (Structured Query Language) is a challenging task in both natural language processing (NLP) and database communities. It involves translating natural language questions into SQL queries that can be executed on a given relational database. This task is essential for enabling non-expert users to interact with databases effectively, as it allows them to pose questions in natural language rather than requiring knowledge of SQL syntax.\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "Traditionally, Text-to-SQL systems relied on predefined rules or query enumeration techniques to handle this task. However, with the advent of deep learning and large language models (LLMs), such as GPT (Generative Pre-trained Transformer) models, there has been a shift towards more data-driven approaches. These models can learn complex patterns and mappings between natural language questions and SQL queries, making them well-suited for Text-to-SQL tasks.\n",
    "\n",
    "The goal of this Text-to-SQL project is to test the DAIL-SQL method. This method  uses the supervised fine-tuning text-to-SQL and prompt engineering empowered by large language models (LLMs) for generating syntactically correct SQL queries.\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Recent advancements in LLM-based Text-to-SQL have shown promising results, with models achieving high accuracy on benchmark datasets like Spider. However, there is still room for improvement, particularly in areas such as prompt engineering, example selection, and fine-tuning of LLMs for this specific task.\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "In this project, we tested the GPT-4 model across two key methodologies from DAIL-SQL. This approach aims to evaluate the effectiveness of these methodologies with a consistent model, allowing for a direct comparison of their performance. The two methodologies are listed below\n",
    "\n",
    "1. DAIL Selection (DAILùëÜ): This method selects examples based on questions and queries, aiming to enhance Text-to-SQL task execution accuracy.\n",
    "2. DAIL Organization (DAILùëÇ): This method organizes examples to balance quality and quantity while preserving question-to-SQL mappings, further improving Text-to-SQL performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Yu et al. (2018) | They used a Seq2Seq model to create a tree based SQL decoder that can identify columns better as well as create nested queries| Spider dataset | Only 48% accuracy\n",
    "| Wang et al. (2020) | They focused on the medical domain considering the unique structure and terminology of medical records. They introduced the Translate-Edit Model for Question-to-SQL (TREQS) generation task by first generating the targeted SQL directly then editing with both attentive-copying mechanism and a recover technique| MIMICSQL | High accuracy of 85% but small dataset of 10k records\n",
    "| Dawei et al. (2023) | They focused on mplementing and refining two key methodologies from DAIL-SQL: DAIL Selection (DAILùëÜ) and DAIL Organization (DAILùëÇ). DAILùëÜ emphasizes candidate example selection based on question and query similarity, while DAILùëÇ focuses on efficient organization of examples while preserving question-SQL mapping. Our aim is to enhance Text-to-SQL systems by optimizing these methods for improved performance and accuracy.| Spider dataset | 86.2% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The study's methodology focuses on exclusively evaluating Text-to-SQL methods using the GPT-4 model, with the Spider datasets. Spider comprises extensive instances split into training and development sets, while Spider-Realistic offers a more challenging subset. \n",
    "\n",
    "In this project, our primary focus was on harnessing the capabilities of the GPT-4 model to enhance Text-to-SQL performance. We tested two key methodologies, DAIL Selection (DAILùëÜ) and DAIL Organization (DAILùëÇ), tailored specifically for GPT-4. DAILùëÜ selects examples based on questions and queries, while DAILùëÇ organizes examples to balance quality and quantity, preserving question-to-SQL mappings. Integrated into our approach, these methodologies aim to significantly elevate Text-to-SQL task execution accuracy.\n",
    "\n",
    "\n",
    "\n",
    "![Alternate text ](src/text-to-sql.png \"Title of the figure, location is simply the directory of the notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "To keep it simple, we have provided bash scripts to run the python modules per task.\n",
    "\n",
    "For best results, it's recommended to prepare the python environment with the requirements file included in the `src` folder and run the modules in terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate python environment.\n",
    "Instructions are in the readme. A requirements_updated.txt file contains the specific software versions for the Python 3.8 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate dail-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "\n",
    "In this step, the Spider dataset is preprocessed.\n",
    "\n",
    "Table schemas are enumerated and question samples from the various databases in the Spider dataset are linked to table schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python data_preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate questions for prediction\n",
    "\n",
    "This process accepts parameters such as \n",
    "* dataset type\n",
    "* LLM for tokenization\n",
    "* prompt representation\n",
    "* k-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_question.py --data_type spider --split test --tokenizer gpt-3.5-turbo --max_seq_len 4096 --prompt_repr SQL --k_shot 9 --example_type QA --selector_type  EUCDISQUESTIONMASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict SQL queries from LLM\n",
    "\n",
    "!python ask_llm.py --openai_api_key YOUR_API_KEY  --model gpt-4 --question prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "In our experiment, we use the DAIL technique which includes 9-shot prompts of SQL query examples as well as hints of the database schema.\n",
    "\n",
    "<img src=\"execution_accuracy_reimplementation.png\" alt=\"execution accuracy for our experiment\" width=\"800\" height=\"480\" />\n",
    "\n",
    "We achieve 95.9% on easier queries but 74.4% overall execution accuracy. Comparatively, the authors of the original paper achieved overall accuracy of 86.2%\n",
    "\n",
    "Evaluation was done using the work of Zhong et all (available on their github repo https://github.com/taoyds/test-suite-sql-eval/tree/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "LLMs require \"coaching\" when they are expected to generate SQL to query an unseen database. Prompt Engineering improves performance of LLMs. The best performance comes from\n",
    "* providing sample queries in the prompt to teach the LLM what is expected - (DAILùëÜ)\n",
    "* providing some level of awareness of the database schema - (DAILùëÇ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. Vaswani, A., et al. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems.\n",
    "\n",
    "2. Zhong, V. W., Xiong, C., & Socher, R. (2017). Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1430-1440)\n",
    "\n",
    "3. Yu, T., Zhang, Z., Gan, Z., Yu, C. Y., & Wang, J. (2018). SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1657-1668).\n",
    "\n",
    "4. Wang, H., Zhang, B., Tu, Z., & Lin, J. (2020). Text-to-SQL generation for question answering on electronic medical records. arXiv preprint arXiv:2009.07307\n",
    "\n",
    "5. Xu, K., Wang, Y., Wang, Y., Wen, Z., & Dong, Y. (2021). SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising. arXiv preprint arXiv:2101.00451.\n",
    "\n",
    "6. Snowflake Blog (2023). Use AI in Seconds with Snowflake Cortex, accessed from https://www.snowflake.com/blog/use-ai-snowflake-cortex/ on March 18, 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
